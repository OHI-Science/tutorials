---
title: "example_prep_non_spatial.rmd"
output: github_document
---


Data prep often takes the biggest chunk of time in an OHI assessment, more than the final calculation of the OHI scores itself! The Prep Folder aims to help you wade through this mundane yet important first step of the assessment. Treat this prep document as your notebook, calculator, and presentation of your work. You can explore raw data, see whether it fits within the bounds of your spatial boundaries and whether it makes sense to include it in your final calculations. If it does, you can format it further and save it as data layers. The process of data exploration is recorded and can be easily shared with anyone. 

**Why prep here?** If someone wants to see where your data comes from, how you have processed the data, and what's the rationale for including or excluding certain data, they can find the answers here. It increases the credibility and reproducibility of your assessment. Prepping in this system will be helpful to yourself. In a few months, or years, you can visit this page and remember what you had done. It also makes your technical team more stable. If there are personale changes, your team memory is preserved. It's easy for any new memeber to pick up where it was left when your data prepation has been documented clearly and kept in one place. 

**Why Github/Rstudio?** To take full advantage of open-science tools, we have found Github and Rstudio to be the best combination for coding and record keeping. If you haven't used this system before, it might seem daunting at first. But this is a highly effective system for teams to work collaboratively, record decisions, and share results easily within or outside your team. Say goodbye to endless email chains and no more passing excel sheets back and forth. 

**Why Rmarkdown?** Rmarkdown produces a report with your comments, R code, and figures/tables embedded in it. It can be rendered as a beautiful HTML webpage to easily communicate with other people. We highly recommend it to anyone who is interested in open science, especially OHI+ users. However, if you prefer to use an r script with comments tucked into it, see `example_prep_non_spatial.r` for the same example.

**What's in this example?** In this document, we'll walk through the general steps we recommend you include in your own prep document, including:

  - introducing the goal/subgoal, goal model, and data 
  - setting up directories and loading libraries
  - loading and formatting data
  - plotting 
  - saving as .csv in layers folder
  
_This example was modified from the prep document for Nutrients subgoal for [OHI-Baltic Clean Water goal](https://github.com/OHI-Science/bhi/blob/draft/baltic2015/prep/CW/secchi/secchi_prep.Rmd). There are two sources of data for secchi depth measurements. At this moment, we don't know which set is more suitable, and whether or how to combine the two sources. You'll see the process of exploring the two data sets and record the observations and subsequent decisions made on how to deal with the two data sets. The r scirpts included commonly used R commands to manipulate and plot data._ 


# Introduction

_In this section you'll introduce the goal/subgoal, what types of information or data are needed, data sources, the goal model, and how to approach trend calculation, etc._

_For example, you can start with a general introduction of what this goal/subgoal is trying to measure, what it means in your local context, and what parameters make sense to be included or explored here._

This subgoal aims to represent the nutrients level in the water in each region. We uses summer time water clarity, measured by secchi depth, as a proxy indicator, assuming a linear relationship between water clarity and nutrient levels. More info can be found [here](http://www.helcom.fi/baltic-sea-trends/indicators/water-clarity). 

## Goal model

_Record what the goal model and reference point should be, how to approach trend calculations, etc._ 

### Status

Xao = Mean Stock Indicator Value / Reference pt

Stock indicators = two HELCOM core indicators assessed for good environemental status (each scored between 0 and 1 by BHI)

Reference pt = maximum possible good environmental status (value=1)

### Trend

_Typically we calculate trend as a linear trend of the last five years of status. In this assessment, however, this approach is not feasible. And an alternative approach is used and documented here._

CPUE time series are available for all stations used for the HELCOM coastal fish populations core indicators. These data were provided by Jens Olsson (FISH PRO II project). To calculate GES status, full time series were used. Therefore, only one status time point and cannot calculate trend of status over time. Instead, we'll follow approach from Bergström et al 2016, but only focus on the final time period for the slope (2004-2013).

Bergstrom et al. 2016. Long term changes in the status of coastal fish in the Baltic Sea. _Estuarin, Coast and Shelf Science_. 169:74-84

Method: 

1. Select final time period of trend assessment (2004-2013)

2. Use time series from both indicators, Key Species and Functional groups. For functional groups,include both cyprinid and piscivore time series

3. For each time series: square-root transform data, z-score, fit linear regression, extract slope

4. Within each time series group (key species, cyprinid, piscivore), take the mean slope for each group within each basin

5. Within each basin take a mean functional group indicator slope (mean of cyprinid mean and piscivore mean)

6. For each basin take overall mean slope - mean of key species and functional group

7. Apply trend value for basin to all BHI regions (except in Gulf of Finland, do not apply Finnish site value to Estonia and Russian regions.)


## Data sources

_Where the data comes from, where it's stored, potential concerns with the data, why you included or excluded certain data, etc:_


**ICE**: Data extracted from database and sent by Hjalte Parner on Feb 10 2016.

_Note from Parner: "extraction from our database classified into HELCOM Assessment Units – HELCOM sub basins with coastal WFD water bodies or water types"_

**SMHI**: Downloaded from SMHI [Shark database](http://www.smhi.se/klimatdata/oceanografi/havsmiljodata/marina-miljoovervakningsdata) on Feb 23 2016 by Lena Viktorsson.

_Download notes: datatyp: Physical and Chemical; Parameter: secchi depth._

_Other comments you could also record here:_

Pros and cons of using these data:

- Pros: these are the most recent published data and thus reflect the most current conditions of water quality... 
- Cons: these datasets don't have full spatial coverage, or don't have even temporal coverage, and thus for some regions we need to do gap filling...

Reasons for exlcuding certain datasets: 

Direct measurements of nutrient levels (eg. phosphate, nitrate, etc) were excluded from this subgoal because not every region measure these chemicals regularly, or we didn't have time-series data on nutrients to be able to calculate trend... 


# Data prep process

## Setup 

This section will set up directories, functions, call commonly used libraries, etc, to prepare for the next steps of data prep. 

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE)

## load common libraries, directories, functions, etc

## Libraries
library(readr)  # install.packages('readr') 
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(tools)

## Directories
dir_scenario = 'github/ohimanual/tutorials' # '~/github/bhi/baltic2015' # this is your scenario 
dir_tutorials = file.path(dir_scenario, 'prep/tutorials') 

dir_prep   = file.path(dir_tutorials, 'data_prep_non_spatial')
dir_goal   # when you have your full repo

```

## Read in data and initial exploration

ICES and SMHI data have non-overlapping observations and were combined to one data set for our use. Some observations were not assigned to any regions (ie. no region IDs attached) and were omitted. 

Both data sets contains profile data (eg temperature, but secchi is only measured once). We need only unique secchi records. Duplicates were thus taken out. 

```{r read in data, echo = FALSE}

#### Read in raw data files 

## read in ices set
data1 = read_csv(file.path(dir_prep, 'example_data', 'ices_secchi.csv'))

## quick review of dataset 
head(data1)
str(data1)

## read in smhi set
data2 = read_csv(file.path(dir_prep, 'example_data', 'smhi_secchi.csv'))

head(data2)
str(data2)

##### NOTE: examples of other functions you could also use for data review
## colnames(data1)
## dim(data1) 

#### Initial filtering

ices <- data1 %>% data.frame() %>%
  select(bhi_id= BHI_ID, secchi, year= Year, month= Month, 
         lat= Latitude, lon = Longitude, date= Date) %>%
  mutate(date = as.Date(date, format= "%Y-%m-%d")) %>%
  mutate(supplier = 'ices')

head(ices)
str(ices)

#### NOTE: " " identifies the package where the function "select" comes from. It's used here because select is also a function from a different package that's loaded  

## which ices data have BHI_ID of NA
ices.na <- ices %>%
           filter(is.na(bhi_id)) 

nrow(ices.na) # counted total of 1684  

## you could do further explorations on why these observations don't have an ID attached. but for simplicity and illustration, 
## we will ignore bhi_id = NA 
ices <- ices %>% 
  filter(!is.na(bhi_id))

###### NOTE: there are other options for data explorations too that are omitted here. For more details, check out section 3.1 of:
###### https://github.com/OHI-Science/bhi/blob/draft/baltic2015/prep/CW/secchi/secchi_prep.Rmd

smhi <- data2 %>% data.frame()%>%
  rename(secchi = value) %>%
  select(bhi_id = BHI_ID, 
         secchi, 
         year = Year, 
         month = Month, 
         lat = Latitude, 
         lon= Longitude, 
         date= Date) %>%
  mutate(supplier = 'smhi') %>% 
  filter(!is.na(bhi_id))
head(smhi)

#### Remove duplicated data within each data set

## is any data duplicated in ices itself
ices.duplicated = duplicated(ices)
sum(ices.duplicated==TRUE) #180963  ## MANY duplicates 

## keep only unique records
new_ices = unique(ices); nrow(new_ices)  #take only unique records # 33018

## is any data duplicated in smhi itself; keeping only unique records
smhi.duplicated = duplicated(select(smhi))
sum(smhi.duplicated==TRUE) #56966 ## MANY duplicates  ## removing station does not affect it
new_smhi = unique(smhi); nrow(new_smhi) #take only unique records # 17090

#### Combine two data sets

## use setdiff() to indentify data smhi not in ices
new_smhi = setdiff(select(new_smhi,-supplier), select(new_ices,-supplier)) %>%
            mutate(supplier = "smhi")
nrow(new_smhi) #10357
## it appears 461 records are duplicates (if remove cruise and station)
## if date, lat, lon, secchi all match, I think they are duplicates

## Now create a new allData, bind only the new_smhi object to ices
allData = bind_rows(new_ices, new_smhi)
nrow(allData) #43253

## double check that there are no duplicates after combining two data sets
allData %>% select(year, month, date, lat, lon,secchi) %>% distinct() %>% nrow(.) #43253

```

## Select only summer obervations 

Only summer months post year 2000 were relevant to our use. Therefore we filtered for data in: 

- Months 6-9 (June, July, August, September)  
- Years 2010-2015

The plots showed that some regions don't have good data coverages. Some basins are missing data for most recent years, such as regions 22 and 25. It appeared that water quality data makes more sense at the basin level, and will be aggregated  to the basin level in the next section. (This is an uncommon case in the Baltic's case, but it's left here for demonstration. )

``` {r select summer data}

summer = allData %>% filter(month %in%c(6:9)) %>%
        filter(year %in% c(2000:2015))
head(summer)

## plot: by month

ggplot(summer) + geom_point(aes(month, secchi, colour=supplier))+
  facet_wrap(~bhi_id, scales ="free_y")

## plot: by year

ggplot(summer) + geom_point(aes(year,secchi, colour=supplier))+
  facet_wrap(~bhi_id)

```

## Combine data to basin level

```{r combine data to basin level}

# read in data to match basin and region  

basin_lookup = read.csv(file.path(dir_prep, 'example_data', 'bhi_basin_country_lookup.csv'), sep=";") %>%
  select(bhi_id = BHI_ID, basin_name=Subbasin)


# add basin name to regions (bhi_id)

summer = summer %>% full_join(., basin_lookup, by="bhi_id")

#Plot by month and by year

ggplot(summer) + geom_point(aes(month,secchi, colour=supplier))+
  facet_wrap(~basin_name)

ggplot(summer) + geom_point(aes(year,secchi, colour=supplier))+
  facet_wrap(~basin_name)

```

## Calculate mean monthly secchi depth by basin

``` {r mean monthly secchi depth}

## calculate monthly means for each month
mean_months = summer %>% select(year, month,basin_name, secchi) %>%
              group_by(year, month, basin_name) %>%
              summarise(mean_secchi = round(mean(secchi, na.rm=TRUE), 1)) %>%
              ungroup()
head(mean_months)

## plot monthly means 
ggplot(mean_months) + geom_point(aes(year,mean_secchi, colour=factor(month))) +
  geom_line(aes(year, mean_secchi, colour=factor(month))) +
  facet_wrap(~basin_name)+
  scale_y_continuous(limits = c(0,10))

## calculate summer means by basin
## basin summer means = mean of basin monthly mean values

mean_months_summer = mean_months %>% select(year, basin_name, mean_secchi) %>%
                      group_by(year, basin_name)%>%
                      summarise(mean_secchi = round(mean(mean_secchi, na.rm=TRUE), 1)) %>%
                      ungroup()  #in mean calculation all some months to have NA, ignore for that years calculation

## plot summer means by basin
ggplot(mean_months_summer) + 
  geom_point(aes(year,mean_secchi)) +
  geom_line(aes(year,mean_secchi))+
  facet_wrap(~basin_name)+
  scale_y_continuous(limits = c(0,10))

```

## Disaggregate data to region level  

Data needs to be deaggregated to the regional level again for status and trend calculations. 


```{r deaggregate to region level}

mean_summer_rgn = mean_months_summer %>% 
  full_join(basin_lookup, by='basin_name') %>% 
 select(rgn_id = bhi_id, year, mean_secchi)

```

ADD NEW CHUCNK: 

```
# setup 

dir_layers = file.path(dir_baltic, 'layers') 

## save data layer in layer folder (this section is commented out since dir_layers is not assigned correctly to your assessment folder...)
# write_csv(mean_summer_rgn, file.path(dir_layers, 'CW_secchi_bhi2015_NJ.csv')) # save as: 'Goal_LayerName_AssessmentName_YourInitial.csv'

```

## _Next steps_

Now you have prepared the data layer(s), you can [register it in `layers.csv`](http://ohi-science.org/manual/#register-data-layers-in-layers.csv). 

_You'll notice that in the original Baltic prep document status and trend were also explored there, instead of functions.r. You could choose to do the same, especially if you want to explore different equations or approaches to trend calculations. We don't always recommend it since this system creates duplicated effortsyou have two places (prep folder and fucntions.r) to update when you make changes to your goal models._

**To render this file and use it to communicate with other human beings**:  

Click on _Knit_ at the top of this window to create an .md file, which can be displayed online as a webpage. Pull/Commit/Push as usual for this file. Click on the .md file in your repo on Github.com to view the rendered file. 

## Step 1: Save and register data layers TODO @ningingj: let's move this into a separate prep Rmd

Before processed data can be used by the Toolbox for calculations, they need to be saved and registered here: 

![](https://docs.google.com/drawings/d/1IwHf8pMBlpzOIYKL0hoEF5rETGA765Qtwkz7es7B3Rg/pub?h=500)

### 1.1 Save data layers in the *layers* folder

When modifying existing or creating new data layers in the prep folder, save the ready-to-use layers in this folder. We recommend saving it as a new *.csv* file with: 

- a prefix identifying the goal (eg. `fis_`)
- a suffix identifying your assessment (eg. `_sc2016.csv`). 

Modifying the layer name provides an easy way to track which data layers have been updated regionally, and which rely on global data. Then, the original layers (`_gl2016.csv` and `_gl2016placeholder.csv`) can be deleted.  

_**Tip**: filenames should not have any spaces: use an underscore instead. This will reduce problems when R reads the files._

### 1.2 Register data layers in `layers.csv`  

After you saved the data layer and named it properly, now you can register it in `layers.csv`. This will tell the Toolbox where to look for data when you calculate scores.  

![](https://docs.google.com/drawings/d/1juueRiA0gevOoHH_8owzLNanyQ-H1mAOEl7L_l5YSJ0/pub?h=600)  

_If a layer simply has a new filename, only the *filename* column needs to be updated._


